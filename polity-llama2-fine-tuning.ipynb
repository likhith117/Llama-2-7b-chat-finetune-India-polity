{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.33.1 trl==0.4.7","metadata":{"execution":{"iopub.status.busy":"2024-04-23T08:06:42.566317Z","iopub.execute_input":"2024-04-23T08:06:42.567149Z","iopub.status.idle":"2024-04-23T08:06:55.225136Z","shell.execute_reply.started":"2024-04-23T08:06:42.567117Z","shell.execute_reply":"2024-04-23T08:06:55.223809Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2024-04-23T08:06:58.435510Z","iopub.execute_input":"2024-04-23T08:06:58.435897Z","iopub.status.idle":"2024-04-23T08:07:18.296302Z","shell.execute_reply.started":"2024-04-23T08:06:58.435863Z","shell.execute_reply":"2024-04-23T08:07:18.295284Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2024-04-23 08:07:06.491090: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-23 08:07:06.491232: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-23 08:07:06.592696: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# The model that you want to train from the Hugging Face hub\nmodel_name = \"NousResearch/Llama-2-7b-chat-hf\"\n\n# The instruction dataset to use\ndataset_name = \"Likhith117/Lakshmikanth_polity_qa\"\n\n# Fine-tuned model name\nnew_model = \"Llama-2-7b-chat-finetune\"\n\n################################################################################\n# QLoRA parameters\n################################################################################\n\n# LoRA attention dimension\nlora_r = 64\n\n# Alpha parameter for LoRA scaling\nlora_alpha = 16\n\n# Dropout probability for LoRA layers\nlora_dropout = 0.1\n\n################################################################################\n# bitsandbytes parameters\n################################################################################\n\n# Activate 4-bit precision base model loading\nuse_4bit = True\n\n# Compute dtype for 4-bit base models\nbnb_4bit_compute_dtype = \"float16\"\n\n# Quantization type (fp4 or nf4)\nbnb_4bit_quant_type = \"nf4\"\n\n# Activate nested quantization for 4-bit base models (double quantization)\nuse_nested_quant = False\n\n################################################################################\n# TrainingArguments parameters\n################################################################################\n\n# Output directory where the model predictions and checkpoints will be stored\noutput_dir = \"./results\"\n\n# Number of training epochs\nnum_train_epochs = 4\n\n# Enable fp16/bf16 training (set bf16 to True with an A100)\nfp16 = False\nbf16 = False\n\n# Batch size per GPU for training\nper_device_train_batch_size = 4\n\n# Batch size per GPU for evaluation\nper_device_eval_batch_size = 4\n\n# Number of update steps to accumulate the gradients for\ngradient_accumulation_steps = 1\n\n# Enable gradient checkpointing\ngradient_checkpointing = True\n\n# Maximum gradient normal (gradient clipping)\nmax_grad_norm = 0.3\n\n# Initial learning rate (AdamW optimizer)\nlearning_rate = 2e-4\n\n# Weight decay to apply to all layers except bias/LayerNorm weights\nweight_decay = 0.001\n\n# Optimizer to use\noptim = \"paged_adamw_32bit\"\n\n# Learning rate schedule\nlr_scheduler_type = \"cosine\"\n\n# Number of training steps (overrides num_train_epochs)\nmax_steps = -1\n\n# Ratio of steps for a linear warmup (from 0 to learning rate)\nwarmup_ratio = 0.03\n\n# Group sequences into batches with same length\n# Saves memory and speeds up training considerably\ngroup_by_length = True\n\n# Save checkpoint every X updates steps\nsave_steps = 0\n\n# Log every X updates steps\nlogging_steps = 25\n\n################################################################################\n# SFT parameters\n################################################################################\n\n# Maximum sequence length to use\nmax_seq_length = None\n\n# Pack multiple short examples in the same input sequence to increase efficiency\npacking = False\n\n# Load the entire model on the GPU 0\ndevice_map = {\"\": 0}","metadata":{"execution":{"iopub.status.busy":"2024-04-23T08:07:32.022861Z","iopub.execute_input":"2024-04-23T08:07:32.023264Z","iopub.status.idle":"2024-04-23T08:07:32.034411Z","shell.execute_reply.started":"2024-04-23T08:07:32.023229Z","shell.execute_reply":"2024-04-23T08:07:32.033204Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Load dataset (you can process it here)\ndataset = load_dataset(dataset_name, split=\"train\")\n\n# Load tokenizer and model with QLoRA configuration\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\n# Check GPU compatibility with bfloat16\nif compute_dtype == torch.float16 and use_4bit:\n    major, _ = torch.cuda.get_device_capability()\n    if major >= 8:\n        print(\"=\" * 80)\n        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n        print(\"=\" * 80)\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=device_map\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n\n# Load LoRA configuration\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\n# Set training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"tensorboard\"\n)\n\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n)\n\n# Train model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T08:07:37.250358Z","iopub.execute_input":"2024-04-23T08:07:37.251196Z","iopub.status.idle":"2024-04-23T10:25:33.018480Z","shell.execute_reply.started":"2024-04-23T08:07:37.251163Z","shell.execute_reply":"2024-04-23T10:25:33.017636Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Downloading data: 100%|██████████| 2.13M/2.13M [00:00<00:00, 15.9MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcf060ec37224082bc9f211f2c2949ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"191d1910d0774e178b2b800e8e99cac0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fe78ae9c3e94f26b3917e1b381fe4d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"609fd2a720ac40f4a1de43ecafb4b3bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f93b0982255c4fcca50bedf4202ee0d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd62337c0a70434aa7b4b5cf9cb1ccde"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"992da86738c2465fa659ba1cbce5d640"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/179 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53045ec71f984cd184f00cad45d42862"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"865eb5926e704231b539fbfbb9ff32c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbac3cbb1e3f45308eaf6a3919d7a68b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1fbf6020c4642a99c7302c73a50d2ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29cc2bb93d764597b68d5aa773249b67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7af1bb420dd8427f8aa048f2037570ed"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/963 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c99df41c4bcf40429d79c37254001b76"}},"metadata":{}},{"name":"stderr","text":"You are using 8-bit optimizers with a version of `bitsandbytes` < 0.41.1. It is recommended to update your version as a major bug has been fixed in 8-bit optimizers.\nYou're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='484' max='484' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [484/484 2:15:44, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>1.843100</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.595700</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.509500</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.467500</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>1.462900</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.449500</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>1.368300</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.379900</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>1.376300</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.343000</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>1.337300</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.341700</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>1.320500</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.291500</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>1.310100</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.278400</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>1.304700</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.320600</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>1.262300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=484, training_loss=1.3983807012069325, metrics={'train_runtime': 8174.9281, 'train_samples_per_second': 0.471, 'train_steps_per_second': 0.059, 'total_flos': 4.75046914572288e+16, 'train_loss': 1.3983807012069325, 'epoch': 4.0})"},"metadata":{}}]},{"cell_type":"code","source":"# Save trained model\ntrainer.model.save_pretrained(new_model)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T10:40:00.478962Z","iopub.execute_input":"2024-04-23T10:40:00.479349Z","iopub.status.idle":"2024-04-23T10:40:00.715991Z","shell.execute_reply.started":"2024-04-23T10:40:00.479320Z","shell.execute_reply":"2024-04-23T10:40:00.715117Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Empty VRAM\n# del model\n# del pipe\ndel trainer\nimport gc\ngc.collect()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T10:42:30.974986Z","iopub.execute_input":"2024-04-23T10:42:30.975670Z","iopub.status.idle":"2024-04-23T10:42:31.739845Z","shell.execute_reply.started":"2024-04-23T10:42:30.975635Z","shell.execute_reply":"2024-04-23T10:42:31.738830Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"# Ignore warnings\nlogging.set_verbosity(logging.CRITICAL)\n\n# Run text generation pipeline with our next model\nprompt = \"What is Article 21?\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2024-04-23T07:35:17.643352Z","iopub.execute_input":"2024-04-23T07:35:17.643715Z","iopub.status.idle":"2024-04-23T07:38:50.887543Z","shell.execute_reply.started":"2024-04-23T07:35:17.643687Z","shell.execute_reply":"2024-04-23T07:38:50.886593Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"<s>[INST] What is Article 21? [/INST]  Article 21 is a part of the Indian Constitution. everybody has the right to life and liberty. No person can be deprived of his life or liberty without due process of law. This article is a part of the fundamental rights chapter of the Constitution.\nThe word ‘life’ in this article means not only the physical existence of a person but also the right to live with dignity. The word ‘liberty’ means not only the freedom from physical restraints but also the freedom from psychological restraints.\nThe word ‘due process of law’ means that the deprivation of life or liberty must be according to the procedure established by the law. This means that the deprivation must be according to the procedure laid down by the Constitution or by any law made by the Parliament or by the state legislature.\nThe word ‘procedure established by the law’ means that the deprivation of life or liberty must be according to the procedure laid down by the Constitution or by any law made by the Parliament or by the state legislature.\nThe word ‘procedure established by the law’ means that the deprivation of life or liberty must be according to the procedure laid down by the Constitution or by any law made by the Parliament or by the state legislature.\nThe word ‘law’ means any law made by the Parliament or by the state legislature.\nThe word ‘due process of law’ means that the deprivation of life or liberty must be according to the procedure laid down by the Constitution or by any law made by the Parliament or by the state legislature.\nThe word ‘procedure established by the law’ means that the deprivation of life or liberty must be according to the procedure laid down by the Constitution or by any law made by the Parliament or by the state legislature.\nThe word ‘procedure established by the law’ means that the deprivation of life or liberty must be according to the procedure laid down by the Constitution or by any law made by the Parliament or by the state legislature.\nThe word ‘procedure established by the law’ means that the deprivation of life or liberty must be according to the procedure laid down by the Constitution or by any law made by the Parliament or by the state legislature.\nThe word ‘procedure established by the law’ means\n","output_type":"stream"}]},{"cell_type":"code","source":"# Reload model in FP16 and merge it with LoRA weights\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=device_map,\n)\nmodel = PeftModel.from_pretrained(base_model, new_model)\nmodel = model.merge_and_unload()\n\n# Reload tokenizer to save it\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-04-23T10:42:36.807918Z","iopub.execute_input":"2024-04-23T10:42:36.808810Z","iopub.status.idle":"2024-04-23T10:43:12.670618Z","shell.execute_reply.started":"2024-04-23T10:42:36.808774Z","shell.execute_reply":"2024-04-23T10:43:12.669631Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37c944590331466399711675bb09a665"}},"metadata":{}}]},{"cell_type":"code","source":"import locale\nlocale.getpreferredencoding = lambda: \"UTF-8\"","metadata":{"execution":{"iopub.status.busy":"2024-04-23T10:43:47.898160Z","iopub.execute_input":"2024-04-23T10:43:47.898552Z","iopub.status.idle":"2024-04-23T10:43:47.903202Z","shell.execute_reply.started":"2024-04-23T10:43:47.898523Z","shell.execute_reply":"2024-04-23T10:43:47.902109Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"!python -c “from huggingface_hub.hf_api import HfFolder; HfFolder.save_token(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)”","metadata":{"execution":{"iopub.status.busy":"2024-04-23T11:01:03.979315Z","iopub.execute_input":"2024-04-23T11:01:03.979718Z","iopub.status.idle":"2024-04-23T11:01:04.982981Z","shell.execute_reply.started":"2024-04-23T11:01:03.979685Z","shell.execute_reply":"2024-04-23T11:01:04.981891Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/bin/bash: -c: line 0: syntax error near unexpected token `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'\n/bin/bash: -c: line 0: `python -c “from huggingface_hub.hf_api import HfFolder; HfFolder.save_token(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)”'\n","output_type":"stream"}]},{"cell_type":"code","source":"kaggle datasets download -p /kaggle/working.zip\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T11:06:41.003737Z","iopub.execute_input":"2024-04-23T11:06:41.004116Z","iopub.status.idle":"2024-04-23T11:06:41.010886Z","shell.execute_reply.started":"2024-04-23T11:06:41.004086Z","shell.execute_reply":"2024-04-23T11:06:41.009514Z"},"trusted":true},"execution_count":31,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[31], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    kaggle datasets download -p /kaggle/working.zip\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (1238065271.py, line 1)","output_type":"error"}]}]}